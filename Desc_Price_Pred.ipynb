{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPa6dJOiYE3hQxVpTTo68N7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import warnings\n","import os\n","import joblib\n","import gc\n","from tqdm import tqdm\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from textblob import TextBlob\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import mean_absolute_error\n","import lightgbm as lgb\n","\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    BASE_PATH = '/content/drive/MyDrive/NLPProject/'\n","    print(\"Google Drive mounted.\")\n","except Exception as e:\n","    print(f\"Not in Colab or Drive mount failed: {e}. Using local paths.\")\n","    BASE_PATH = './'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qo9Lk4MVH8_B","executionInfo":{"status":"ok","timestamp":1762501270353,"user_tz":-330,"elapsed":25998,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"9484ace5-e8e3-496f-e521-de06e3970be8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Google Drive mounted.\n"]}]},{"cell_type":"code","source":["data_path = os.path.join(BASE_PATH, 'train.csv')\n","embeddings_path = os.path.join(BASE_PATH, 'train_embeddings.parquet') # Assuming this is your CLIP parquet\n","MODEL_SAVE_PATH = BASE_PATH # Save models to the same folder\n","\n","try:\n","    df = pd.read_csv(data_path)\n","    print(f\"Dataset: {df.shape}\")\n","\n","    img_embeddings = pd.read_parquet(embeddings_path)\n","    print(f\"Image embeddings: {img_embeddings.shape}\")\n","    clip_col_name = 'clip_embedding'\n","    if clip_col_name not in img_embeddings.columns:\n","        raise ValueError(f\"Column '{clip_col_name}' not found in embeddings parquet.\")\n","\n","    df = df.merge(img_embeddings[['sample_id', clip_col_name]], on='sample_id', how='left')\n","    df = df.rename(columns={clip_col_name: 'clip'})\n","    print(f\"Merged dataset: {df.shape}\")\n","\n","except FileNotFoundError as e:\n","    print(f\"Error loading files: {e}\")\n","    df = pd.DataFrame()\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")\n","    df = pd.DataFrame()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gseG4VMcH_8G","executionInfo":{"status":"ok","timestamp":1762501340516,"user_tz":-330,"elapsed":70169,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"f78b14b8-0e76-4a71-abb2-4da50f3af833"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: (75000, 4)\n","Image embeddings: (74999, 3)\n","Merged dataset: (75000, 5)\n"]}]},{"cell_type":"code","source":["def parse_item_name(t):\n","    if pd.isna(t): return \"\"\n","    m = re.search(r'Item Name:\\s*(.+?)(?:\\n|$)', t)\n","    return m.group(1).strip() if m else \"\"\n","\n","def parse_product_desc(t):\n","    if pd.isna(t): return \"\"\n","    m = re.search(r'Product Description:\\s*(.+?)(?:\\n(?:Bullet Point|Value|Unit|$))', t, re.DOTALL)\n","    return m.group(1).strip() if m else \"\"\n","\n","def parse_value(t):\n","    if pd.isna(t): return np.nan\n","    m = re.search(r'Value:\\s*([0-9.]+)', t)\n","    return float(m.group(1)) if m else np.nan\n","\n","def extract_brand(name):\n","    if pd.isna(name) or not name: return \"unknown\"\n","    words = name.split()\n","    if words and len(words[0]) > 2:\n","        return words[0]\n","    return \"unknown\"\n","\n","def extract_pack_size(text):\n","    if pd.isna(text): return 1\n","    patterns = [r'\\(Pack of (\\d+)\\)', r'Pack of (\\d+)', r'(\\d+)-Pack']\n","    for p in patterns:\n","        m = re.search(p, text, re.IGNORECASE)\n","        if m: return int(m.group(1))\n","    return 1\n","\n","def clean_text(t):\n","    if pd.isna(t): return \"\"\n","    t = t.lower()\n","    t = re.sub(r'\\(pack of \\d+\\)', '', t)\n","    t = re.sub(r'pack of \\d+', '', t)\n","    t = re.sub(r'\\d+-pack', '', t)\n","    t = re.sub(r'\\d+ count', '', t)\n","    t = re.sub(r'\\d+\\.?\\d*\\s*(oz|ounce|lb|pound|gram|kg|ml|liter|fl oz)', '', t)\n","    t = re.sub(r'[^a-z0-9\\s]', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t).strip()\n","    return t\n","\n","print(\"Feature engineering functions defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L2rVYpGqOmpl","executionInfo":{"status":"ok","timestamp":1762501340516,"user_tz":-330,"elapsed":52,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"487ac3f3-efa4-428f-9581-fd495a2e4fbd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature engineering functions defined.\n"]}]},{"cell_type":"code","source":["if not df.empty:\n","    print(\"Applying feature engineering...\")\n","    df['item_name'] = df['catalog_content'].apply(parse_item_name)\n","    df['desc'] = df['catalog_content'].apply(parse_product_desc)\n","    df['parsed_value'] = df['catalog_content'].apply(parse_value)\n","\n","    df['full_text'] = (df['item_name'].fillna('') + ' ' + df['desc'].fillna(''))\n","    df['full_text_cleaned'] = df['full_text'].apply(clean_text)\n","\n","    df['item_name_length'] = df['item_name'].str.len().fillna(0)\n","    df['desc_length'] = df['desc'].str.len().fillna(0)\n","    df['has_description'] = (df['desc_length'] > 0).astype(int)\n","\n","    df['brand'] = df['item_name'].apply(extract_brand)\n","    df['brand_count'] = df['brand'].map(df['brand'].value_counts()).fillna(0)\n","\n","    df['pack_size'] = df['item_name'].apply(extract_pack_size)\n","    df['total_volume'] = df['parsed_value'].fillna(0)\n","    df['log_pack_size'] = np.log1p(df['pack_size'])\n","    df['log_total_volume'] = np.log1p(df['total_volume'])\n","\n","    df['log_price'] = np.log1p(df['price'])\n","    print(\"Feature engineering complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RDXAgI1hOqVt","executionInfo":{"status":"ok","timestamp":1762501345881,"user_tz":-330,"elapsed":5368,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"74ae0596-df5e-4be1-ea4e-0f52c56332c3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Applying feature engineering...\n","Feature engineering complete.\n"]}]},{"cell_type":"code","source":["if not df.empty and 'clip' in df.columns:\n","    print(\"Processing CLIP embeddings...\")\n","\n","    # Handle missing values\n","    missing_count = df['clip'].isna().sum()\n","    df['has_clip'] = (~df['clip'].isna()).astype(int)\n","    print(f\"Found {missing_count} missing CLIP embeddings.\")\n","\n","    def parse_embedding(embed_obj):\n","        if pd.isna(embed_obj):\n","            return np.zeros(512)\n","        if isinstance(embed_obj, str):\n","            try:\n","                clean_str = embed_obj.strip('[]')\n","                return np.fromstring(clean_str, sep=',')\n","            except:\n","                return np.zeros(512)\n","        if hasattr(embed_obj, '__iter__'):\n","            try:\n","                return np.asarray(embed_obj, dtype=float)\n","            except:\n","                 return np.zeros(512)\n","\n","        return np.zeros(512)\n","    print(\"Converting embeddings column to numpy matrix...\")\n","    embeddings_matrix = np.stack(df['clip'].apply(parse_embedding).values)\n","    embedding_dim = embeddings_matrix.shape[1]\n","\n","    if embedding_dim != 512:\n","        print(f\"Warning: Detected embedding dimension is {embedding_dim}, not 512.\")\n","\n","    clip_cols = [f'clip_{i}' for i in range(embedding_dim)]\n","    clip_df = pd.DataFrame(embeddings_matrix, columns=clip_cols, index=df.index)\n","    df = pd.concat([df, clip_df], axis=1)\n","\n","    print(f\"CLIP embeddings processed: {embedding_dim} dimensions.\")\n","    del clip_df, embeddings_matrix\n","    gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65_VJogSPhvD","executionInfo":{"status":"ok","timestamp":1762501363215,"user_tz":-330,"elapsed":17332,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"b3d02791-614e-415b-d2a1-82631e08642a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing CLIP embeddings...\n","Found 1 missing CLIP embeddings.\n","Converting embeddings column to numpy matrix...\n","CLIP embeddings processed: 512 dimensions.\n"]}]},{"cell_type":"code","source":["def smape(y_true, y_pred):\n","    y_true_orig = np.expm1(y_true)\n","    y_pred_orig = np.expm1(y_pred)\n","\n","    denom = (np.abs(y_true_orig) + np.abs(y_pred_orig))\n","    diff = np.abs(y_true_orig - y_pred_orig) / denom\n","    diff[denom == 0] = 0.0\n","    return 100 * np.mean(diff)\n","\n","def mae(y_true, y_pred):\n","    y_true_orig = np.expm1(y_true)\n","    y_pred_orig = np.expm1(y_pred)\n","    return mean_absolute_error(y_true_orig, y_pred_orig)\n","\n","print(\"Metrics defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCp5PnkaPiar","executionInfo":{"status":"ok","timestamp":1762501363252,"user_tz":-330,"elapsed":31,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"1671da88-c494-4fcc-e70c-53acd00da712"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Metrics defined.\n"]}]},{"cell_type":"code","source":["if not df.empty:\n","    y = df['log_price']\n","    drop_cols = ['price', 'log_price', 'catalog_content', 'image_link', 'sample_id',\n","                 'item_name', 'desc', 'full_text', 'clip']\n","\n","    X = df.drop(columns=[c for c in drop_cols if c in df.columns])\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n","    del df\n","    gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3ZQwQXsPq68","executionInfo":{"status":"ok","timestamp":1762501363713,"user_tz":-330,"elapsed":459,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"0ce0dfa6-cebb-4484-e4fa-9dc786f7899a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Train: (60000, 524), Test: (15000, 524)\n"]}]},{"cell_type":"code","source":["numeric_features = ['parsed_value', 'pack_size', 'total_volume',\n","                    'brand_count', 'item_name_length', 'desc_length',\n","                    'has_description', 'log_pack_size', 'log_total_volume', 'has_clip']\n","categorical_features = ['brand']\n","text_feature = 'full_text_cleaned'\n","clip_features = [col for col in X_train.columns if col.startswith('clip_')]\n","\n","# Ensure all features exist\n","numeric_features = [f for f in numeric_features if f in X_train.columns]\n","categorical_features = [f for f in categorical_features if f in X_train.columns]\n","\n","print(f\"Using {len(numeric_features)} numeric features.\")\n","print(f\"Using {len(categorical_features)} categorical features.\")\n","print(f\"Using {len(clip_features)} CLIP features.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqlvOH-IPrac","executionInfo":{"status":"ok","timestamp":1762501363725,"user_tz":-330,"elapsed":10,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"5a2983eb-30de-46bb-8af3-1c85df0458ea"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using 10 numeric features.\n","Using 1 categorical features.\n","Using 512 CLIP features.\n"]}]},{"cell_type":"code","source":["numeric_transformer = Pipeline([\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', RobustScaler())\n","])\n","\n","categorical_transformer = Pipeline([\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore', max_categories=50))\n","])\n","\n","text_transformer = TfidfVectorizer(\n","    stop_words='english',\n","    max_features=5000,\n","    ngram_range=(1, 2)\n",")\n","\n","text_preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features),\n","        ('tfidf', text_transformer, text_feature)\n","    ],\n","    remainder='drop'\n",")\n","\n","# Create the full pipeline\n","text_model = Pipeline([\n","    ('preprocessor', text_preprocessor),\n","    ('model', lgb.LGBMRegressor(\n","        n_estimators=1000,\n","        learning_rate=0.05,\n","        num_leaves=31,\n","        n_jobs=-1,\n","        random_state=42,\n","        objective='huber'\n","    ))\n","])\n","\n","# Train\n","print(\"Fitting text_model...\")\n","text_model.fit(X_train, y_train)\n","\n","# Evaluate\n","pred_text = text_model.predict(X_test)\n","smape_text = smape(y_test, pred_text)\n","mae_text = mae(y_test, pred_text)\n","print(f\"Text Model SMAPE: {smape_text:.4f}%\")\n","print(f\"Text Model MAE: ${mae_text:.4f}\")\n","\n","# Save\n","text_model_path = os.path.join(MODEL_SAVE_PATH, 'text_model.pkl')\n","joblib.dump(text_model, text_model_path)\n","print(f\"Text model saved to {text_model_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_a9R4FrOPwHt","executionInfo":{"status":"ok","timestamp":1762501556853,"user_tz":-330,"elapsed":193126,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"6b8b6df7-994a-4e26-d596-b118940efa33"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting text_model...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 3.661375 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 431601\n","[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 5048\n","[LightGBM] [Info] Start training from score 2.740904\n","Text Model SMAPE: 26.8093%\n","Text Model MAE: $12.1466\n","Text model saved to /content/drive/MyDrive/NLPProject/text_model.pkl\n"]}]},{"cell_type":"code","source":["image_preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ],\n","    remainder='passthrough'\n",")\n","\n","clip_indices = [X_train.columns.get_loc(c) for c in clip_features if c in X_train]\n","non_clip_indices = [X_train.columns.get_loc(c) for c in numeric_features + categorical_features if c in X_train]\n","\n","image_preprocessor_v2 = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features),\n","        ('clip', StandardScaler(), clip_features)\n","    ],\n","    remainder='drop'\n",")\n","\n","image_model = Pipeline([\n","    ('preprocessor', image_preprocessor_v2),\n","    ('model', lgb.LGBMRegressor(\n","        n_estimators=1000,\n","        learning_rate=0.05,\n","        num_leaves=31,\n","        n_jobs=-1,\n","        random_state=42,\n","        objective='huber'\n","    ))\n","])\n","\n","# Train\n","print(\"Fitting image_model...\")\n","image_model.fit(X_train, y_train)\n","\n","# Evaluate\n","pred_image = image_model.predict(X_test)\n","smape_image = smape(y_test, pred_image)\n","mae_image = mae(y_test, pred_image)\n","print(f\"Image Model SMAPE: {smape_image:.4f}%\")\n","print(f\"Image Model MAE: ${mae_image:.4f}\")\n","\n","# Save\n","image_model_path = os.path.join(MODEL_SAVE_PATH, 'image_model.pkl')\n","joblib.dump(image_model, image_model_path)\n","print(f\"Image model saved to {image_model_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zGy3ZKiVP2x1","executionInfo":{"status":"ok","timestamp":1762501726508,"user_tz":-330,"elapsed":169651,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"5cbdbf3f-4b64-46fb-b9bf-9f6704c3d0b2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting image_model...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.979587 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 132211\n","[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 571\n","[LightGBM] [Info] Start training from score 2.740904\n","Image Model SMAPE: 27.8563%\n","Image Model MAE: $12.7837\n","Image model saved to /content/drive/MyDrive/NLPProject/image_model.pkl\n"]}]},{"cell_type":"code","source":["pred_ensemble = (pred_text + pred_image) / 2.0\n","\n","smape_ensemble = smape(y_test, pred_ensemble)\n","mae_ensemble = mae(y_test, pred_ensemble)\n","\n","print(f\"Text Model SMAPE:   {smape_text:.4f}% | MAE: ${mae_text:.4f}\")\n","print(f\"Image Model SMAPE:  {smape_image:.4f}% | MAE: ${mae_image:.4f}\")\n","print(f\"Ensemble SMAPE: {smape_ensemble:.4f}% | MAE: ${mae_ensemble:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJrkuFvfQDmf","executionInfo":{"status":"ok","timestamp":1762501726520,"user_tz":-330,"elapsed":9,"user":{"displayName":"divyagnan","userId":"18029102493881806617"}},"outputId":"d1b57379-1d0d-4733-9382-7147dd9f01b5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Text Model SMAPE:   26.8093% | MAE: $12.1466\n","Image Model SMAPE:  27.8563% | MAE: $12.7837\n","Ensemble SMAPE: 26.6320% | MAE: $12.1415\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tiHb7ybHQWld"},"execution_count":null,"outputs":[]}]}